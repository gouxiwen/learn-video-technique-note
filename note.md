## 1. 基本概念

1. 像素
2. 分辨率
3. 位深

- RGB 图像中每个颜色占用的位数，如 8 位

4. Stride（跨距）

- 为了快速读取一行像素，对内存中存的图像进行对齐，如 16 字节对齐，当一个图像的一行像素不能被 16 整除时就要进行填充

5. 帧率
6. 码率

- 单位：位/秒，对于文件，文件总大小/时长，对于流，每秒占用的宽带，并不是码率越大视频越清晰，和压缩算法和压缩速度有关

## 2. RGB 和 YUV 颜色空间

- RGB
  - RGB 用红绿蓝三种颜色来表示一个像素，摄像头和显示器都使用 RGB 格式颜色空间，RGB 三种颜色具有相关性，三种颜色同等重要，且要保持同步
- YUV
  - YUV 用一个亮度 Y 和两个颜色 U 和 V 来表示像素，Y 和 UV 的没有相关性，可以分开编码，广泛应用于视频领域，视频编码一般用的是 YUV 图像
  - 按照表示一个像素需要的 YUV 的比例有 4:4:4,4:2:2,4:2:0 三种类型，相同分辨率的图像使用 4:2:2 和 4:2:0 可以占用更少的空间
  - 每种类型按照存储方式又细分了多种
- RGB 和 YUV 的转换
  - 转换标准主要有 BT601 和 BT709，BT601 是标清的标准，而 BT709 是高清的标准
  - Color Range 有两种，Full Range 的 R、G、B 取值范围都是 0 ～ 255。而 Limited Range 的 R、G、B 取值范围是 16 ～ 235。
  - RGB 和 YUV 之间的转换要先明确 YUV 的存储格式、转换标准和 Color Range，根据转换标准和 Color Range 会有四种转换公式

## 3. 图像缩放算法

最常用的插值算法和目前比较火的 AI 超分算法

### 插值算法

1. 按照比例将目标像素点映射到原图像的坐标
2. 映射的坐标一般是小数，这时候就需要插值计算到的
3. 三种插值算法：

- 最近邻插值，取周围 4 个源像素，直接取最近的像素值
  - 优点：计算速度快
  - 缺点：计算出的像素有大概率相同，使得放大是出现块状，缩小时出现锯齿
- 双线性插值，取周围 4 个源像素，进行三次线性计算得到像素值，线性计算就是根据距离的远近分配权重计算颜色值
  - 优缺点：比最近邻插值慢一些，效果好一些
- 双三次插值，取周围 16 个源像素，通过 BiCubic 基函数计算每个像素的权重，分别在 x，y 方向上计算出权重相乘得到每个点的最终权重，然后根据每个点权重求和得到目标点的像素

  - BiCubic 基函数

    <img src="https://static001.geekbang.org/resource/image/70/18/702c29205ca2542fd57a5ffc9961e118.jpg?wh=1280x717" width="50%">

  - x，y 方向上分别求权重

    假设源像素坐标为 p（x，y），目标坐标为 p（u，v），通过上面公式可以求得其水平权重 W（u - x），垂直权重 W（v - y），则该源像素计算出的颜色值为： W（u - x）* W（v - y）*p（x，y）

  - 求和 16 个点

    <img src="https://static001.geekbang.org/resource/image/51/ba/5111be20665a5cdecd9e08a00b6b11ba.jpg?wh=1280x720" width="50%">

## 4. 编码原理

- 通过帧内预测消除空间冗余
- 通过帧间预测消除时间冗余
- 通过 DCT 变换和量化消除视觉冗余
- 通过熵编码消除信息熵冗余

I 帧不能进行帧间预测，B 帧和 P 帧既可以帧间预测又可以帧内预测，编码器针对每一帧会选择一种最优的方式

## 5. 码流结构

### 视频图像序列的层次结构

- 帧类型
  - I 帧：自身完成编码解码，压缩率小
  - P 帧：向前编码，压缩率较 I 帧高
  - B 帧：即可向前编码又可向后编码，压缩率高，延迟大，RTC 场景不适用
  - IDR 帧：一种特殊的 I 帧，H264 编码标准中规定，IDR 帧之后的帧不能再参考 IDR 帧之前的帧
- GOP（图像组）
  - 两个 IDR 帧之间的帧是一个 GOP，GOP 越大说明 IDR 帧越少，压缩率越高，但是如果由于网络不好出现丢包则会出现长时间花屏
  - GOP 不是越大越好，也不是越小越好，需要根据实际场景来选择

### 图像内部层次结构

- slice（切片）：一个图像分成多个切片，切片互相独立，帧内预测不能跨切片
- MB（宏块）：一个切片里有整数个宏块
- 子块：一个宏块可以分成若个个子，用来给复杂区域做精细化编码

### H264 的码流结构

_从外向内研究_

#### 1.码流中图像数据的最小单位是 slice

#### 2.SPS、PPS 和 SEI

除了图像数据，视频编码的时候还有一些编码参数数据，为了能够将一些通用的编码参数提取出来，不在图像编码数据中重复，H264 设计了两个重要的参数集：一个是 SPS（Sequence Parameter Set，序列参数集）；一个是 PPS（Picture Parameter Set，图像参数集）。

其中，SPS 主要包含的是图像的宽、高、YUV 格式和位深等基本信息；PPS 则主要包含熵编码类型、基础 QP 和最大参考帧数量等基本编码信息。如果没有 SPS、PPS 里面的基础信息，之后的 I 帧、P 帧、B 帧就都没办法进行解码。因此 SPS 和 PPS 是至关重要的。

除了 SPS 和 PPS，还有一种叫 SEI，SEI 中一般存储了 H.264 中的一些附加信息

#### 3.H264 码流有两种格式：一种是 Annexb 格式；一种是 AVCC（AVC1、MPEG-4） 格式

它们的区别是用来标识码流的标识符不同，Annexb格式 用起始码，有 4 个字节和 3 个字节的两种，MP4格式 是用一个 4 字节的长度标识

Annexb格式H264 的码流结构就是 起始码+编码数据+起始码+编码数据...... // 通常用于直播流
AVCC格式H264 的码流结构就是 长度标识+编码数据+长度标识+编码数据...... // 通常用于本地文件播放

#### 4.什么是 NALU？

NALU（网络抽象层单元），由于 H264 码流主要由 SPS、PPS、SEI 和三种 slice 组成，为了在码流中区分这几种数据，H264 设计了 NALU，因此一个 SPS 是一个 NALU，一个 PPS 是一个 NALU，一个 slice 也是一个 NALU，这个NALU就是编码数据。

对于本地码流，可能只要在第一个IDR NALU之前出现SPS和PPS就就可以，但是对于直播流每个IDR 帧前必须插入SPS和PPS，这样不论什么时候开始解码都可以正确解码。

直播码流 = 起始码+SPS NALU + 起始码+PPS NALU + 若干个（起始码+IDR NALU） + 若干个（起始码+非 IDR NALU） + 起始码+SPS NALU + 起始码+PPS NALU + 若干个（起始码+IDR NALU ）+ 若干个（起始码+非 IDR NALU） + ......

本地码流 = 起始码+SPS NALU + 起始码+PPS NALU + 若干个（起始码+IDR NALU） + 若干个（起始码+非 IDR NALU） + 若干个（起始码+IDR NALU ）+ 若干个（起始码+非 IDR NALU） + ......

NALU = NALU Header + NALU Data，通过 NALU Header 就可以区分不同类型的 NALU（SPS、PPS、IDR 帧）

slice NALU Data = slice Header + slice Data，通过 slice Header 可以区分 P 帧、B 帧、普通的 I 帧

slice Date = MB + MB + ......

## 5. 帧内预测（减少空间冗余）

帧内预测有多种模式，根据图像复杂度可以分成不同大小的块，16x16，8x8，4x4，在编码过程中会根据预测值和实际的差值，计算出一种最优的模式，这个模式会在解码时用于预测解码，先计算出预测值，再加上差值就是解码后的值。

## 6. 帧间预测（减少时间冗余）

一般 RTC 场景用编码帧的上一帧作为参考帧，在上一帧中通过算法找出预测块，原则就是编码块和预测块差值的绝对值之和（SAD）最小

### 运动矢量

当两帧图像中的某一块内容一样但是位置不一样时，用运动矢量来表示编码帧中编码块和参考帧中的预测块之间的位置的差值，解码的时候使用运动矢量在参考帧中找预测块

### 运动搜索

两帧中的某一块内容有没有运动通过眼睛很容易判断，但是编码器没有眼睛，需要通过运动搜索算法判断，运动搜索就是在上一帧中遍历宏块，然后找到 SAD 最小的块作为参考块

- 全搜索算法
  在上一帧中遍历所有宏块及子块，比较耗时
- 钻石搜索算法
  也叫菱形搜索算法，先以菱形中心点算 SAD，再分别计算四个顶点的 SAD，找出最小的 SAD，如果最小的是中心点，则搜索停止，否则以最小的顶点为中心点继续搜索
- 六边形搜索算法
  基本原理和菱形一样，不同在于找到中心点以后要继续以中心点为中心继续做菱形和矩形搜索

#### 如何确定搜索起点？

通过相邻已编码的运动矢量来预测

#### 如何处理半像素及 1/4 像素？

图像的运动不一定是一整块像素的，这种情况下，可以使用亚像素插值+亚像素精度运动搜索作为一种候选算法

##### 亚像素插值+亚像素精度运动搜索

原理是将原本一个个的整像素，通过插值算法算出半个像素和 1/4 个像素，这样即使运动矢量不是一整个像素也能找到准确的位置。

亚像素插值得到的预测块并不一定就比整像素预测块的残差小。只是我们多了很多个半像素预测块和 1/4 像素预测块的选择，所以我们可以在整像素预测块、半像素预测块和 1/4 像素预测块里面选择一个最好的。

1. 亚像素插值
   通过亚像素插值算法得到一幅亚像素的帧

2. 亚像素精度运动搜索
   结合整像素运动搜多会得到一个最优的运动矢量

### 运动矢量来预测

确定了运动矢量以后，不会直接编码在编码信息中，而是先用周围相邻块的运动矢量预测一个预测运动矢量，称为 MVP。将当前运动矢量与 MVP 的残差称之为 MVD，然后编码到码流中去的，就像编码块帧内预测那样。

### SKIP 模式

MVD 为(0，0)，同时，残差块经过变换量化后系数也都是等于 0，那么当前编码块的模式就是 SKIP。

### 帧间模式的选择

编码块帧间模式的选择其实就是参考帧的选择、运动矢量的确定，以及块大小（也就是块划分的方式）的选择，如果 SKIP 单独拿出来算的话就再加上一个判断是不是 SKIP 模式。

### 最终预测模式的选择

选择了帧间模式以后还要继续和帧内预测做比较，最终选出一个最好的模式来

## 7.DCT 变换和量化（减少视觉冗余）

利用率人眼对高频信号不敏感的原理，去除无用的高频信号

### DCT 变换

通过数学公式将图像信号转换到频域上表示

### Hadamard 变换

一种比 DCT 变换更快速的转换方法，避免了 DCT 变换的浮点运算，因此更快，用来快速估算编码后的大小，并不能真正代替 DCT

### 量化

量化就是一个相除的过程，得到更多的 0，被除数 QStep 的大小决定了压缩率和压缩后的质量

### H264 中的 DCT 变换和量化

H264 为了减少这种浮点型运算漂移带来的误差，将 DCT 变换改成了整数变换，DCT 变换中的浮点运算和量化过程合并，这样就只有一次浮点运算过程

## 7.RTP 和 RTCP

1. RTP 协议传递音视频数据

2. RTCP 协议用来辅助 RTP 协议，RTCP 报文有很多种，分别负责不同的功能。常用的报文有发送端报告（SR）、接收端报告（RR）、RTP 反馈报告（RTPFB）等。

   RTCP 协议只是用来传递 RTP 包的传输统计信息，本身不具有丢包重传和带宽预测的功能，而这些功能需要我们自己来实现。

### H264 RTP 打包

RTP H264 码流打包分为三种方式：分别是单 NALU 封包方式、组合封包方式、分片封包方式

- 单 NALU 封包
  一个 NALU 打包成一个 RTP 包
- 组合封包
  多个 NALU 打包成一个 RTP 包
- 分片封包
  一个 NALU 拆成多个分别打包成 RTP 包

## 8.带宽预测

带宽预测，顾名思义，就是实时预测当前的网络带宽大小。预测出实际的带宽之后，我们就可以控制音视频数据的发送数据量

### 两种网络设备类型

- 有较大缓存的
  前者在网络中需要转发数据过多的时候，会把数据先缓存在自己的缓冲队列中，等待前面的数据发送完之后再发送当前数据。这种情况就会在网络带宽不够的时候，需要当前数据等一段时间才能发送，因此表现出来的现象就是网络不好时，延时会加大。
- 没有缓存或者缓存很小的
  后者在网络中需要发送的数据过多的时候，会直接将超过带宽承受能力的数据丢弃掉。这种情况就会在网络带宽不够的时候，出现高丢包的现象。

### WebRTC 中的三种带宽预测算法

### 1. 基于延时的带宽预测算法（适用于有较大缓存的网络设备）

- 计算延时
  对于一组 RTP 包，分别计算出发送端的时长和接收端的时长，将接收时长减去发送时长就是延时
- 延时变化的趋势计算
  使用滤波器--Trendline Filter 进行计算
- 网络状态判断
  使用过载检测器，两个作用：1.过载、欠载还是正常状态。2.更新延时阈值
- 带宽调整更新
  使用速率控制器

### 2.基于丢包的带宽预测算法（适用于没有缓存或者缓存很小的网络设备）

- 丢包率的计算
- 带宽调整

### 3.最大带宽探测算法

为了防止预测带宽时发送码流较小时突然突然升高码流导致预估带宽不够而限制发送码率（即使实际网络带宽足够），从而导致画面出现模糊和马赛克等问题，这种情况一般发生在程序刚启动或者画面由静止突然转为运动状态，WebRTC 会计算出一个最大带宽并且更新之前计算的预估带宽。

- 程序刚启动会探测一下
- 每隔一段时间会探测一下

## 9. 码率控制

### 1.码控的原理

码控是编码器的一个模块，但不是编码标准，是编码器自己实现的程序。
那码控的原理是什么呢？其实码控就是为每一帧编码图像选择一个合适的 QP 值的过程。我们知道当一帧图像的画面确定了之后，画面的复杂度和 QP 值几乎决定了它编码之后的大小。由于编码器无法决定画面的复杂度，因此，码控的目标就是选择一个合适的 QP 值，以此来控制编码后码流的大小。

### 2.码控的类型

常用的码控算法主要有：VBR（动态码率）、CQP（恒定 QP）、CRF（恒定码率因子）和 CBR（恒定码率）这几种。
前三种本质上都是动态码率，可以认为 CQP 和 CRF 是特殊的 VBR，适用于点播和短视频。
CBR 的恒定码率是指给编码器一个目标码率，而不是从始至终保持恒定，这种方式会配合宽带算法实时地调整目标码率使得接近于预估的宽带值。

### 3.CBR 算法

由于码率的大小由复杂度和 QP 两个因素，目标码率已知，因此 CBR 算法总体分两步：1.计算画面复杂度，2.计算 QP

#### 1.复杂度求解

复杂度的计算需要用到方差，这里的方差并不是真的去计算方差，而是通过一些算法预估一个近似值。

- I 帧复杂度求解
  I 帧使用帧内预测，因此用宏块的方差计算宏块的复杂度，再将宏块的复杂度求和去计算帧的复杂度。
- P 帧复杂度求解
  P 帧使用帧间预测，因此使用当前帧和参考帧对应宏块的 SAD 作为宏块复杂度，求和计算帧复杂度。

以上帧内每个宏块的复杂度都会记录

#### 2.计算 QP

计算 QP 分两步：1.计算帧级 QP 值，2.H264 中还根据已编码的宏块组 GOM 进一步调整未编码的 GOM。

- 帧组级
  根据帧率和目标码率计算每一帧的目标大小，帧组中的每一帧大小都会根据已编码的帧进行实时计算的。
- 帧级
  当前帧的复杂度和目标大小，结合前面已编码帧的复杂度和 QStep 以及使用这个 QStep 实际编码的大小来估算当前帧的 QStep。
- GOM 级
  先计算一下帧的实际剩余大小和帧的目标剩余大小，然后将帧的实际剩余大小除以帧的目标剩余大小，如果这个比例大于 1，说明我们剩余的大小多了，之后的 GOM 可以将 QP 调低一些，我们将后面的 GOM 中的宏块 QP 值减去 1 或者 2 即可；如果这个比例小于 1，说明我们剩余的大小少了，之后的 GOM 的 QP 需要调高一些，我们将后面的 GOM 中的宏块 QP 值加 1 或者 2 即可。

## 10.Jitter Buffer 处理花屏卡顿问题

接收端在收到 RTP 包以后先经过 Jitter Buffer 进行处理再进行解码。

### 卡顿问题

如果两帧之间的播放时间间隔超过了 200ms，人眼就可以明显看出卡顿了。

#### 1.帧率不够

采集的帧率或者设置的帧率低于 5fps 时，即使均匀播放，也会卡顿。
解决方法：提高帧率到 15fps 甚至更高

#### 2.机器性能不够，导致前处理或者编码耗时太长

前处理：缩放，图像增强，美颜
编码：码控，预测，变换，量化，熵编码
解决方法：GPU 处理前处理工作，编码器设置为低档（压缩率会下降）

#### 4.编码器输出码率超过实际网络带宽

当图像比较复杂且编码码率较高时容易出现发送码率大于实际宽带，就会丢弃多出的包，导致接收端无法正常解码
解决方法：CBR 的码控算法

#### 5.复杂帧编码后过大或者 I 帧比较大

即使使用了 CBR 码控算法，如果突然出现一个很大的帧也会导致编码很大
解决方法：平滑发送模块，将一个帧大量 RTP 包均匀地发送出去，而不是一次性发送出去，这个模块在 WebRTC 中叫做 PacedSender（节奏发送器）。

#### 6.网络本身就有一定的丢包率

解决方法：Jitter Buffer 发送丢包重传请求
丢包重传请求策略是在 Jitter Buffer 里面实现的。当接收端接收到视频 RTP 包之后，会检查 RTP 序列号。
通过比对 RTP 序列号，记录丢包列表，周期性地通过 RTCP 协议中的 NACK 报文发给发送端

#### 7.重传也没有收到包

多次丢包重传后依然没有收到包，就需要请求一帧完整的 I 帧，即关键帧，前面课中我们讲到过，如果有一个帧解码失败，那之后的帧几乎都将解码失败，直到下一个 IDR 帧到来。
解决方法：Jitter Buffer 使用 RTCP 协议中的 FIR 报文请求关键帧。

### 花屏问题

#### 1.帧不完整

解决方法：Jitter Buffer 中来对帧进行完整性判断的。

#### 2.参考帧不完整

解决方法：Jitter Buffer 中来对帧进行完整性判断的。

#### 3.YUV 格式问题

解决方法：渲染时使用正确的 YUV

#### 4.Stride 问题

解决方法：渲染时使用正确的 Stride

## 11.SVC 实现视频编码可伸缩

### 1.为什么需要 SVC

在一对多通话场景下，多个接收端的网络情况不一样，这样就需要发送的编码出不同大小的码流，这就需要 SVC。

### 2.什么是 SVC

SVC 是指一个码流当中，我们可以分成好几层。比如说分成三层：

- 第 0 层是最底层，可以独立进行编解码，不依赖第 1 层和第 2 层；
- 第 1 层编解码依赖于第 0 层，但是不依赖于第 2 层；
- 第 2 层的编解码需要依赖于第 0 层和第 1 层；

并且，第 0 层质量最低，第 0 层加第 1 层次之，三层加在一起的时候质量最高。注意这里的质量不是直接指的画面质量，而是帧率、分辨率的高低所代表的质量。

### 3.SVC 的分类

#### 1.时域 SVC

时域 SVC 是指在帧率上做 SVC

##### 原理

在帧间预测编码中，P 帧编码时需要参考前一帧，时域 SVC 就是通过调整参考帧的方式将帧分成不同层，从而实现分层发送依然可以解码。

##### 缺点：

一般自然运动是连续的，选择前一帧作为参考帧一般压缩率会比较高，因为前后相邻的两帧很相似。而时域 SVC 这种跨帧参考的方式会使得压缩率有一定的下降。两层 SVC 编码效率大概下降 10%，三层大概下降 15%。

#### 2.空域 SVC

时域 SVC 是指在分辨率上做 SVC

##### 原理

将每一帧编码成不同分辨率实现分层

##### 缺点

这种多分辨率的空域 SVC 相比多个编码器编码不同分辨率的方式，在压缩率上也没有多少优势，而且还不符合常规的标准。H264、H265、VP8 这些常用的编码标准（除了扩展）都是不支持空域 SVC 的。

因此，在 WebRTC 中直接使用多个编码器编码多种分辨率的方式代替空域 SVC。

#### 3.时域 SVC 如何实现可伸缩

VP8 的 RTP 协议在 RTP 头和 VP8 码流数据的中间还有一个 RTP 描述头，这个描述头主要用来放帧号、层号等信息的。

服务器可以从 RTP 描述头得到 RTP 包对应的层号。这样服务器就可以通过 RTP 的层号和 RTP 的包大小来估算每一层的码率了。

而接收端可以根据帧号、层号和层同步标志位等信息来判断当前帧是不是可以解码，而不用去解码视频码流。可以解码才能送解码器，不然就不能送去解码，防止出现花屏。

## 12.视频的封装格式--MP4 & FLV

### 1.FLV

FLV 封装也是比较简单的封装格式，它是由一个个 Tag 组成的。Tag 又分为视频 Tag、音频 Tag 和 Script Tag，分别用来存放视频数据、音频数据和 MetaData 数据。

### 2.MP4

MP4 由一个个 box 组成，每一个 box 存放了不同的数据，而且 box 里面还可以嵌套着 box。

总结：flv 结构简单稳定，没有索引之类的这种东西。适合流媒体。

## 13.音画同步

### 1.PTS 和 DTS

PTS 表示的是视频帧的显示时间，DTS 表示的是视频帧的解码时间。对于同一帧来说，DTS 和 PTS 可能是不一样的。因为 B 帧可以双向参考，可以参考后面的 P 帧，那么就需要将后面用作参考的 P 帧先编码或解码，然后才能进行 B 帧的编码和解码。

### 2.时间基

时间基是什么呢？很简单，它就是时间的单位。
RTP 的时间戳，它的单位是 1/90000 秒
FLV 封装，时间基是 1/1000

### 3.音视频同步的类型

#### 1.视频同步到音频--常用

原理就是在播放中通过比较视频时钟和音频时钟的差值，动态地调整视频帧的播放时间，从而达到音画同步。
具体

- 首先，我们使用的时间戳是 PTS，因为播放视频的时间我们应该使用显示时间。而且我们需要先通过时间基将对应的时间戳转换到常用的时间单位，一般是秒或者毫秒。
- 然后，我们有一个视频时钟和一个音频时钟来记录当前视频播放到的 PTS 和音频播放到的 PTS。注意这里的 PTS 还不是实际视频帧的 PTS 或者音频帧的 PTS，稍微有点区别。
  区别是什么呢？比如说一帧视频的 PTS 的 100s，这一帧视频已经在渲染到屏幕上了，并且播放了 0.02s 的时间，那么当前的视频时钟是 100.02s。也就是说视频时钟和音频时钟不仅仅需要考虑当前正在播放的帧的 PTS，还要考虑当前正在播放的这一帧播放了多长时间，这个值才是最准确的时钟。
  而视频时钟和音频时钟的差值就是不同步的时间差。这个时间差我们记为 diff，表示了当前音频和视频的不同步程度。我们需要做的就是尽量调节来减小这个时间差的绝对值。
- 最后调整播放时间，使 dfff 尽量接近 0，做不到完全同步。

#### 2.音频同步到视频--不常用

因为人耳的敏感度很高，相对于视频来说，音频的调整更容易被人耳发现。

#### 3.音频和视频都做调整同步--WebRtc 使用
